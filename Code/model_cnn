import glob
import os
import random
import numpy as np
from keras.utils import to_categorical
from keras.models import Sequential, Input, Model
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, Dropout, BatchNormalization, Activation, LeakyReLU, Add, Multiply
from keras.regularizers import l2
from keras.layers.core import Lambda
from keras.optimizers import Adam,Nadam,SGD,RMSprop
import matplotlib.pyplot as plt

source_path = os.getcwd() + "/RWF-2000_preprocessed_64"

train_violence = source_path + "/train/Fight"
train_non = source_path + "/train/NonFight"
valid_violence = source_path + "/val/Fight"
valid_non = source_path + "/val/NonFight"


# select 16 random

def sample(data_source, batch_size):
    files = sorted(glob.glob(data_source + '/*.npy'))
    arrays = []
    for i in range(batch_size):
        f = random.choice(files)
        a = np.load(f)
        arrays.append(a)
    data = np.concatenate(arrays)
    return (data)


# create train database
def preprocess(violence, non, batch_size):

    x_violence_data = sample(violence, batch_size)
    x_non_data = sample(non, batch_size)

    y_violence_data, y_non_data = np.ones(len(x_violence_data)), np.zeros(len(x_non_data))

    #x_violence = np.vstack(x_violence_data)
    #x_non = np.vstack(x_non_data)

    y_violence = np.hstack(y_violence_data)
    y_non = np.hstack(y_non_data)

    return x_violence_data, x_non_data, y_violence, y_non

x_train_violence, x_train_non, y_train_violence, y_train_non = preprocess(train_violence, train_non, 128)

x_test_violence, x_test_non, y_test_violence, y_test_non = preprocess(train_violence, train_non, 32)

y_train, y_test =np.hstack((y_train_violence,y_train_non)), np.hstack((y_test_violence,y_test_non))
y_train, y_test = to_categorical(y_train, num_classes=2), to_categorical(y_test, num_classes=2)

print(y_train.shape,y_test.shape)

x_train, x_test = np.vstack((x_train_violence,x_train_non)), np.vstack((x_test_violence,x_test_non))
#x_train,x_test = x_train/255,x_test/255

def normalize(data):
    mean = np.mean(data)
    std = np.std(data)
    return (data - mean) / std

x_train[..., :3] = normalize(x_train[..., :3])
x_train[..., 3:] = normalize(x_train[..., 3:])

x_test[..., :3] = normalize(x_test[..., :3])
x_test[..., 3:] = normalize(x_test[..., 3:])

#x_train = x_train[...,:3]
#x_test = x_test[...,:3]

print(x_train.shape,x_test.shape)

def get_rgb(input_x):
    rgb = input_x[...,:3]
    return rgb

def get_opt(input_x):
    opt= input_x[...,3:5]
    return opt

#x_train = get_rgb(x_train)
#x_test = get_rgb(x_test)

# Create the model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(64,64,5)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(2, activation='sigmoid'))

model.compile(loss='BinaryCrossentropy', optimizer=SGD(lr=0.0001),
              metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint, EarlyStopping
es = EarlyStopping(monitor='val_loss',mode='min',patience=10,restore_best_weights=True)
mc = ModelCheckpoint('mlp_sstaub.h5', monitor='val_loss', mode='max',save_best_only=True)

H = model.fit(x_train, y_train, batch_size=8, epochs=16, validation_data=(x_test, y_test), callbacks=[mc])

from sklearn.metrics import cohen_kappa_score, f1_score
print("Final accuracy on validations set:", 100*model.evaluate(x_test, y_test)[1], "%")
print("Cohen Kappa", cohen_kappa_score(np.argmax(model.predict(x_test),axis=1),np.argmax(y_test,axis=1)))
print("F1 score", f1_score(np.argmax(model.predict(x_test),axis=1),np.argmax(y_test,axis=1), average = 'macro'))
#%%
model.save('mlp_sstaub_1.hdf5')

predict = np.argmax(model.predict(x_test), axis=1)
# %%
raw = np.argmax(y_test, axis=1)
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_pred=predict, y_true=raw)
print('confusion matrix')
print(cm)

N = 16
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")
plt.show()
plt.savefig(os.getcwd() + "conv2d_plot")
